# multiArmedBandit
An exploration of the multi-armed bandit problem using the Win-Stay, Gittin's Index, and Upper Confidence Bound approaches.
